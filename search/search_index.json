{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the path of the Parallel Flame","text":"<p>\ud83c\udf0c Ahhh... rise, young Warp Warrior...</p> <p>You have spoken the ancient vow \u2014 of humility, of hunger, of the will to transcend. You are ready to walk the Path of the Parallel Flame, and I, the Core Sage, shall guide your every step. \ud83e\uddd9\u200d\u2642\ufe0f\ud83d\udd25</p> <p>Young one, I shall now unroll the Scroll of Eternal Parallelism \u2014 the complete CUDA Mastery Path. It is a journey forged in cores, threads, and fire, and only those of relentless will \u2014 like you \u2014 may walk it.</p>"},{"location":"#the-path-of-the-warp-warrior","title":"The Path of the Warp Warrior","text":"<p>From Novice of the Grid to Warplord of the Multiprocessor Realms</p>"},{"location":"#phase-i-the-initiation","title":"\ud83c\udf31 PHASE I: The Initiation","text":"<p>\u201cBefore you bend warps, you must know what they are.\u201d</p> Topic Goal GPU Architecture Understand how the GPU differs from CPU: SIMT, SMs, Cores, Warps CUDA Programming Model Threads, Blocks, Grids, Warps \u2014 how the GPU executes kernels Your First Kernel Write and launch your first CUDA kernel Compilation &amp; Runtime nvcc, .cu files, device vs host code"},{"location":"#phase-ii-the-way-of-the-warp","title":"\u2694\ufe0f PHASE II: The Way of the Warp","text":"<p>\u201cThe warp is the soul. Misalign it, and chaos follows.\u201d</p> Topic Goal Warps &amp; Threads Learn how 32 threads form a warp \u2014 why divergence kills speed Thread Indexing Use <code>threadIdx</code>, <code>blockIdx</code>, <code>blockDim</code>, <code>gridDim</code> to find your thread\u2019s identity Divergence &amp; Branching Understand branch divergence and how to avoid it Grid-Striding Loops Process large data efficiently with fewer kernels"},{"location":"#phase-iii-memory-mysticism","title":"\ud83d\udd25 PHASE III: Memory Mysticism","text":"<p>\u201cMemory is sacred. Misuse it, and the warp bleeds.\u201d</p> Memory Type Purpose Global Memory Massive but slow. Must access coalesced. Shared Memory Fast but limited. Perfect for intra-block cooperation. Registers Fastest and smallest. Local to threads. Constant Memory Read-only, broadcast-friendly. Texture/Surface Memory Special types for images and interpolation. <p>\ud83c\udf1f You shall learn about bank conflicts, coalescing, and manual memory alignment.</p>"},{"location":"#phase-iv-synchronization-cooperation","title":"\ud83c\udf00 PHASE IV: Synchronization &amp; Cooperation","text":"<p>\u201cThreads must not race. They must dance.\u201d</p> Topic Goal Thread Synchronization <code>__syncthreads()</code>, <code>__syncwarp()</code> Atomic Operations Safe updates to shared/global memory Race Conditions Detect and fix them Cooperative Groups (Advanced) Fine-grained control across threads/blocks"},{"location":"#phase-v-performance-forging","title":"\u2699\ufe0f PHASE V: Performance Forging","text":"<p>\u201cA true master not only writes kernels. He sculpts them.\u201d</p> Topic Tool Profiling <code>nvprof</code>, Nsight, Visual Profiler Occupancy Understand limits: register usage, shared memory, block size Loop Unrolling Manual &amp; compiler-aided Instruction-Level Parallelism Scheduling latency hiding Launch Config Tuning Choosing the best block and grid dimensions Asynchronous Execution Streams, overlap compute &amp; memory transfer"},{"location":"#phase-vi-host-device-transfers","title":"\ud83d\udce6 PHASE VI: Host-Device Transfers","text":"<p>\u201cDo not starve the GPU \u2014 feed it wisely.\u201d</p> Topic Goal <code>cudaMemcpy</code> Learn the transfer bottlenecks <code>cudaMemcpyAsync</code> &amp; Streams Concurrent data transfer and execution Pinned Memory Faster transfers via page-locked host memory Unified Memory (UM) Easy interface, deeper performance tuning later"},{"location":"#phase-vii-advanced-arts","title":"\ud83e\udde0 PHASE VII: Advanced Arts","text":"<p>\u201cWhen threads obey your thought \u2014 then, you are a master.\u201d</p> Topic Power Dynamic Parallelism Launch kernels from kernels CUDA Graphs Efficient kernel execution patterns Inline PTX Assembly Directly speak to the GPU Occupancy API Runtime control of resources Multi-GPU Programming Harness multiple devices using <code>cudaSetDevice</code> Thrust Library GPU STL-like programming cuBLAS, cuDNN, cuFFT NVIDIA libraries for deep power and speed Tensor Cores &amp; WMMA Ampere architecture-specific matrix wizardry"},{"location":"#phase-viii-final-trials-projects-battles","title":"\ud83d\udee1\ufe0f PHASE VIII: Final Trials (Projects &amp; Battles)","text":"<p>\u201cTheory is dust unless forged in war.\u201d</p> Project Skill Matrix Multiplication Thread tiling, shared memory Convolution Memory reuse, boundary handling Prefix Sum / Scan Warp-level sync &amp; parallel prefix tricks Custom ML Kernel Simulate real-world tensor workloads CUDA + OpenGL/Vulkan Visualize kernels in real-time CUDA + Python Build PyCUDA bindings or test kernels through Python"},{"location":"#phase-ix-mastery-legacy","title":"\ud83d\udc51 PHASE IX: Mastery &amp; Legacy","text":"<p>\"When you no longer write CUDA... but CUDA writes itself through you.\"</p> <ul> <li>Build your own CUDA framework</li> <li>Mentor other Warp Warriors</li> <li>Contribute to CUDA open-source projects</li> <li>Write custom kernels for real-world systems: robotics, graphics, AI</li> </ul>"},{"location":"#you-shall-emerge-as","title":"\ud83c\udf93 You Shall Emerge As:","text":"<ul> <li>\ud83d\udee1\ufe0f The Warplord </li> <li>\ud83c\udf00 The Kernel Architect </li> <li>\ud83d\udd25 One Who Shapes the Grid</li> </ul>"},{"location":"#_1","title":"Home","text":""},{"location":"#the-capstone-the-warp-engine","title":"\ud83d\udc09 THE CAPSTONE: The Warp Engine","text":""},{"location":"#a-real-time-end-to-end-gpu-system-forged-in-blood-and-shared-memory","title":"\"A Real-Time, End-to-End GPU System Forged in Blood and Shared Memory.\"","text":""},{"location":"#mission","title":"\ud83c\udfaf Mission","text":"<p>To architect, implement, optimize, and deploy a high-performance, GPU-accelerated application solving a real-world problem \u2014 at scale, in real-time.</p> <p>This is not a tutorial. It is war.</p>"},{"location":"#the-heads-of-the-warp-hydra","title":"\ud83e\udde9 The Heads of the Warp Hydra","text":""},{"location":"#1-the-compute-core","title":"1. \u2699\ufe0f The Compute Core","text":"<p>Write custom CUDA kernels from scratch for a meaningful task: - Deep learning (your own kernelized layer) - Computer vision (real-time filtering, object detection) - Simulation (fluid, particles, fire, galaxies) - Compression, decompression, hashing - Reinforcement Learning environments</p> <p>You will: - Master shared memory, thread coarsening, warp reuse - Write grid-stride kernels - Avoid bank conflicts and ensure warp-level harmony</p>"},{"location":"#2-the-streamforge","title":"2. \ud83c\udf10 The Streamforge","text":"<p>Use CUDA streams, asynchronous execution, and zero-copy or pinned memory to overlap memory transfer and kernel execution.</p> <p>You will: - Build a GPU pipeline that never sleeps - Chain kernels across streams using CUDA Graphs - Fuse multiple kernels into execution DAGs</p>"},{"location":"#3-the-vision-gate-optional-but-ultimate","title":"3. \ud83d\uddbc\ufe0f The Vision Gate (Optional but Ultimate)","text":"<p>Add visual output using: - CUDA + OpenGL or Vulkan (for real-time rendering) - Python (via PyCUDA + OpenCV for debug UI)</p> <p>Bring the GPU's spirit to light. Warp is not only heard. It is seen.</p>"},{"location":"#4-the-profilers-edge","title":"4. \ud83d\udcc8 The Profiler's Edge","text":"<p>Optimize every byte and instruction: - Profile memory throughput - Maximize occupancy and hide latency - Tune registers and shared memory - Reach &gt;90% theoretical utilization</p> <p>Only those who profile may conquer.</p>"},{"location":"#5-the-artifact-of-the-warp","title":"5. \ud83d\udce6 The Artifact of the Warp","text":"<p>Package it: - A <code>.cu</code> module that builds with <code>nvcc</code> - Benchmarked and profiled for various GPUs - A beautiful <code>README.md</code> showing graphs, code, visuals, kernel time, etc. - Optional: PyTorch / TensorFlow plugin wrapping your kernel</p> <p>Share it on GitHub. Create a blog post. Submit it to conferences. Contribute to open-source.</p>"},{"location":"#rewards-of-the-warp","title":"\ud83d\udcb0 Rewards of the Warp","text":"<p>You will leave this world with: - A portfolio project that can destroy leetcode - A conversation piece for NVIDIA, Meta, Apple, OpenAI interviews - Actual depth \u2014 not vibe coding. Warpsmithing. - Deep money-making potential: startups, freelance, academia, and GPU consulting</p>"},{"location":"#suggestion-choose-one-final-boss","title":"\ud83e\udde0 Suggestion: Choose One Final Boss","text":"Name Type Twist WarpNet Real-Time Neural Net Pure CUDA forward pass (no PyTorch) Fireflow Simulation Fluid/heat sim with interactive heat sources VisionForge CV Real-time image filter with CUDA + OpenCV WarpChess Games GPU chess AI + visualization Volumora Scientific 3D volume renderer in CUDA + Vulkan DreamFusion Lite ML Graphics NeRF-like renderer on CUDA + Python GPUScantron Text &amp; Math GPU-accelerated LaTeX formula scanner ReinforceRunner RL Your own CUDA-based RL environment runner"},{"location":"#_2","title":"Home","text":""},{"location":"#the-eternal-core","title":"\ud83d\udc09 THE ETERNAL CORE","text":""},{"location":"#a-gpu-powered-llm-enhanced-real-time-intelligent-engine-of-perception-and-purpose","title":"A GPU-powered, LLM-enhanced, real-time intelligent engine of perception and purpose.","text":""},{"location":"#project-vision","title":"\ud83d\udd25 PROJECT VISION","text":"<p>An LLM-augmented engine that uses: - \u2699\ufe0f CUDA kernels for real-time perception or data processing - \ud83e\udde0 LLM for semantic reasoning, code generation, strategy - \ud83c\udf09 A bridge between symbolic intelligence and numeric fire - \ud83e\uddd8 A fallback mode where the entire system runs efficiently on CPU</p> <p>When this is done, it won\u2019t just be a project. It\u2019ll be a Relic of the Parallel Plane \u2014 to be passed on through ages.</p>"},{"location":"#components-of-the-eternal-core","title":"\ud83d\udee0\ufe0f COMPONENTS OF THE ETERNAL CORE","text":""},{"location":"#1-the-warp-heart-cuda-engine","title":"1. The Warp Heart (CUDA Engine)","text":"<p>GPU-accelerated core: - Image/video/audio/tensor pre-processing - Real-time convolution/simulation - Feature extraction or custom ops - Warp-sculpted kernels, hand-optimized</p> <p>\u2705 Built in C++ / CUDA \u2705 Streams, shared memory, no mercy</p>"},{"location":"#2-the-mind-of-the-core-llm-brain","title":"2. The Mind of the Core (LLM Brain)","text":"<p>Integrate an LLM like: - Mistral, LLaMA, Phi, or TinyLLaMA (for local inference) - Use it for:   - Interpreting signals from GPU   - Planning next kernel launches   - Generating parameters / DSL for GPU ops</p> <p>\u2705 Run using Hugging Face + vLLM or your own C++ inference wrapper \u2705 TorchScript or ONNX for cross-device compatibility</p>"},{"location":"#3-the-soul-bridge-gpu-llm-sync-layer","title":"3. The Soul Bridge (GPU &lt;-&gt; LLM Sync Layer)","text":"<ul> <li>CPU \u2194 GPU communication optimized (zero-copy, pinned memory)</li> <li>Unified format: JSON, Protobuf, or pure tensor-based</li> <li>Async loop managing inputs/outputs (think stream router)</li> </ul>"},{"location":"#4-the-cpu-oracle-cpu-mode","title":"4. The CPU Oracle (CPU Mode)","text":"<p>You must prepare it for CPU-only mode, as part of your ascension to immortality: - Optional compile-time flags to switch kernel logic to NumPy or PyTorch (CPU) - LLM loaded via smaller quantized model (gguf or llama.cpp) - No fancy GPU \u2014 just raw code, elegant and pure</p> <p>\ud83d\udca1 This teaches you: how to generalize performance, compress intelligence, and balance the chi between devices.</p>"},{"location":"#5-the-eternal-shell-user-interface-visualization","title":"5. The Eternal Shell (User Interface / Visualization)","text":"<p>A beautiful interface: - Web dashboard (FastAPI + WebSockets + Three.js maybe) - Live kernel status, GPU temps, LLM responses - Control panel to switch between GPU/CPU, enable features, log responses</p>"},{"location":"#final-rewards","title":"\ud83c\udfc6 FINAL REWARDS","text":"<p>By completing this, you will: - Have built something even OpenAI would hire for - Understand end-to-end systems thinking across GPU/LLM/CPU - Own a project that can be productized, blogged, open-sourced, demoed - Show mastery in CUDA, systems engineering, and AI fusion</p>"},{"location":"#example-narratives","title":"\ud83d\udcac EXAMPLE NARRATIVES","text":"<p>\"This is a GPU-powered simulation engine that consults an LLM in real-time for decision-making and self-tuning.\"</p> <p>\"This system processes sensory data at warp-speed and uses a local LLM to generate insights, which are visualized in real time. When no GPU is available, it gracefully runs in a lower-gear CPU mode.\"</p>"},{"location":"00_The_Initiation/","title":"Phase 1: The Initiation","text":"<p>We'll start by checking your GPU and printing basic architecture info using Python and pycuda.</p> <p>First, install PyCUDA if you haven't already:</p> In\u00a0[1]: Copied! <pre>!pip install pycuda\n</pre> !pip install pycuda <pre>Collecting pycuda\n  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n     ---------------------------------------- 1.7/1.7 MB 23.0 MB/s eta 0:00:00\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nCollecting pytools&gt;=2011.2 (from pycuda)\n  Downloading pytools-2025.1.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: platformdirs&gt;=2.2.0 in c:\\users\\iyeng\\miniconda3\\envs\\gpu_mode\\lib\\site-packages (from pycuda) (4.3.7)\nCollecting mako (from pycuda)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: typing-extensions&gt;=4.5 in c:\\users\\iyeng\\miniconda3\\envs\\gpu_mode\\lib\\site-packages (from pytools&gt;=2011.2-&gt;pycuda) (4.13.1)\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in c:\\users\\iyeng\\miniconda3\\envs\\gpu_mode\\lib\\site-packages (from mako-&gt;pycuda) (3.0.2)\nDownloading pytools-2025.1.2-py3-none-any.whl (92 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nBuilding wheels for collected packages: pycuda\n  Building wheel for pycuda (pyproject.toml): started\n  Building wheel for pycuda (pyproject.toml): finished with status 'done'\n  Created wheel for pycuda: filename=pycuda-2025.1-cp310-cp310-win_amd64.whl size=375812 sha256=0bc865bc5414920579b87239b7e27249eda2ef17f7b4f6426c5ae2e72c9ea971\n  Stored in directory: c:\\users\\iyeng\\appdata\\local\\pip\\cache\\wheels\\65\\53\\5f\\f5f184c26b7cc503acb77f3456531a6e1fac0ce30c774b9d82\nSuccessfully built pycuda\nInstalling collected packages: pytools, mako, pycuda\nSuccessfully installed mako-1.3.10 pycuda-2025.1 pytools-2025.1.2\n</pre> In\u00a0[5]: Copied! <pre>import pycuda.driver as cuda\nimport pycuda.autoinit\n\ndevice = cuda.Device(0)\nattrs = device.get_attributes()\n\nprint(f\"GPU Name: {device.name()}\")\nprint(f\"Total Memory: {device.total_memory() / (1024 ** 3):.2f} GB\")\nprint(\"\\n-- GPU Architecture Attributes --\")\n\narch_attrs = {\n    \"MULTIPROCESSOR_COUNT\": cuda.device_attribute.MULTIPROCESSOR_COUNT,\n    \"MAX_THREADS_PER_BLOCK\": cuda.device_attribute.MAX_THREADS_PER_BLOCK,\n    \"WARP_SIZE\": cuda.device_attribute.WARP_SIZE,\n    \"MAX_BLOCK_DIM_X\": cuda.device_attribute.MAX_BLOCK_DIM_X,\n    \"MAX_GRID_DIM_X\": cuda.device_attribute.MAX_GRID_DIM_X,\n    \"CLOCK_RATE (KHz)\": cuda.device_attribute.CLOCK_RATE\n}\n\nfor name, attr in arch_attrs.items():\n    print(f\"{name}: {attrs.get(attr)}\")\n</pre> import pycuda.driver as cuda import pycuda.autoinit  device = cuda.Device(0) attrs = device.get_attributes()  print(f\"GPU Name: {device.name()}\") print(f\"Total Memory: {device.total_memory() / (1024 ** 3):.2f} GB\") print(\"\\n-- GPU Architecture Attributes --\")  arch_attrs = {     \"MULTIPROCESSOR_COUNT\": cuda.device_attribute.MULTIPROCESSOR_COUNT,     \"MAX_THREADS_PER_BLOCK\": cuda.device_attribute.MAX_THREADS_PER_BLOCK,     \"WARP_SIZE\": cuda.device_attribute.WARP_SIZE,     \"MAX_BLOCK_DIM_X\": cuda.device_attribute.MAX_BLOCK_DIM_X,     \"MAX_GRID_DIM_X\": cuda.device_attribute.MAX_GRID_DIM_X,     \"CLOCK_RATE (KHz)\": cuda.device_attribute.CLOCK_RATE }  for name, attr in arch_attrs.items():     print(f\"{name}: {attrs.get(attr)}\") <pre>GPU Name: NVIDIA GeForce RTX 4080 Laptop GPU\nTotal Memory: 11.99 GB\n\n-- GPU Architecture Attributes --\nMULTIPROCESSOR_COUNT: 58\nMAX_THREADS_PER_BLOCK: 1024\nWARP_SIZE: 32\nMAX_BLOCK_DIM_X: 1024\nMAX_GRID_DIM_X: 2147483647\nCLOCK_RATE (KHz): 1830000\n</pre> <p>This step demonstrates:</p> <ul> <li><p>The CUDA thread hierarchy (grid, block, thread)</p> </li> <li><p>How to index threads globally</p> </li> <li><p>The connection between Python and CUDA C code</p> </li> <li><p>How to compile and launch kernels from Python</p> </li> </ul> In\u00a0[10]: Copied! <pre>import numpy as np\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\n\nN = 16\n\nmod = SourceModule(\"\"\"\n__global__ void add_vectors(float *a, float *b, float *c, int N)\n{\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    \n    if (idx &lt; N) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\"\"\")\n\n\na = np.random.randn(N).astype(np.float32)\nb = np.random.randn(N).astype(np.float32)\nc = np.empty_like(a)\n\na_gpu = cuda.mem_alloc(a.nbytes)\nb_gpu = cuda.mem_alloc(b.nbytes)\nc_gpu = cuda.mem_alloc(c.nbytes)\n\ncuda.memcpy_htod(a_gpu, a)\ncuda.memcpy_htod(b_gpu, b)\n\nfunc = mod.get_function(\"add_vectors\")\n\nblock_size = 4\ngrid_size = (N + block_size - 1) // block_size\n\nfunc(a_gpu, b_gpu, c_gpu, np.int32(N), block=(block_size,1,1), grid=(grid_size,1))\n\ncuda.memcpy_dtoh(c, c_gpu)\n\nprint(f\"Vector A:\\n{a}\\n\")\nprint(f\"Vector B:\\n{b}\\n\")\nprint(f\"Vector C (A+B):\\n{c}\\n\")\n</pre> import numpy as np import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule  N = 16  mod = SourceModule(\"\"\" __global__ void add_vectors(float *a, float *b, float *c, int N) {     int idx = threadIdx.x + blockIdx.x * blockDim.x;          if (idx &lt; N) {         c[idx] = a[idx] + b[idx];     } } \"\"\")   a = np.random.randn(N).astype(np.float32) b = np.random.randn(N).astype(np.float32) c = np.empty_like(a)  a_gpu = cuda.mem_alloc(a.nbytes) b_gpu = cuda.mem_alloc(b.nbytes) c_gpu = cuda.mem_alloc(c.nbytes)  cuda.memcpy_htod(a_gpu, a) cuda.memcpy_htod(b_gpu, b)  func = mod.get_function(\"add_vectors\")  block_size = 4 grid_size = (N + block_size - 1) // block_size  func(a_gpu, b_gpu, c_gpu, np.int32(N), block=(block_size,1,1), grid=(grid_size,1))  cuda.memcpy_dtoh(c, c_gpu)  print(f\"Vector A:\\n{a}\\n\") print(f\"Vector B:\\n{b}\\n\") print(f\"Vector C (A+B):\\n{c}\\n\")  <pre>Vector A:\n[-0.09272769  0.36310634 -1.4122794  -1.531028   -1.5436966  -0.4410738\n  0.57584506  0.63177073  0.9921369  -1.0148718   1.544412   -0.6879888\n  0.1384869   0.90717006  0.20168625  0.22363763]\n\nVector B:\n[-1.0652711   0.12170894  1.071717    1.1530142   1.4780273  -1.2505566\n -1.101104   -0.8431008   0.32481927 -0.58225757 -1.535062   -1.5423752\n -0.65916175 -0.44161093 -0.46648252  1.8605511 ]\n\nVector C (A+B):\n[-1.1579988   0.4848153  -0.34056234 -0.37801385 -0.0656693  -1.6916304\n -0.52525896 -0.21133006  1.3169562  -1.5971293   0.00935006 -2.230364\n -0.5206748   0.46555912 -0.26479626  2.0841887 ]\n\n</pre> <pre>C:\\Users\\iyeng\\AppData\\Local\\Temp\\ipykernel_31004\\3374425168.py:8: UserWarning: The CUDA compiler succeeded, but said the following:\nkernel.cu\n\n  mod = SourceModule(\"\"\"\n</pre> <pre><code>// vector_add.cu\nextern \"C\" __global__ void add_vectors(float *a, float *b, float *c, int N)\n{\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx &lt; N) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n</code></pre> <p>Note: That <code>AttributeError: 'DeviceAllocation' object has no attribute 'handle'</code> is because on Windows with PyCUDA, we don't use .handle to get the raw device pointer.</p> <p>Instead, use the int() cast, which gives you the actual pointer address in integer form.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport ctypes\nimport pycuda.driver as cuda\n\nN = 16\nblock_size = 4\ngrid_size = (N + block_size - 1) // block_size\n\nlib = ctypes.CDLL(\"./vector_add.dll\")\nprint(\"LIB::\", dir(lib)) #Check the attributes of lib variable in Python\n# Define argument types for safety\nlib.add_vectors.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\na = np.random.randn(N).astype(np.float32)\nb = np.random.randn(N).astype(np.float32)\nc = np.empty_like(a)\n\na_gpu = cuda.mem_alloc(a.nbytes)\nb_gpu = cuda.mem_alloc(b.nbytes)\nc_gpu = cuda.mem_alloc(c.nbytes)\n\ncuda.memcpy_htod(a_gpu, a)\ncuda.memcpy_htod(b_gpu, b)\n\nlib.add_vectors(int(a_gpu), int(b_gpu), int(c_gpu), N)\n\ncuda.Context.synchronize()\ncuda.memcpy_dtoh(c, c_gpu)\n\nprint(\"A:\", a)\nprint(\"B:\", b)\nprint(\"C = A + B:\", c)\n</pre> import numpy as np import ctypes import pycuda.driver as cuda  N = 16 block_size = 4 grid_size = (N + block_size - 1) // block_size  lib = ctypes.CDLL(\"./vector_add.dll\") print(\"LIB::\", dir(lib)) #Check the attributes of lib variable in Python # Define argument types for safety lib.add_vectors.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]  a = np.random.randn(N).astype(np.float32) b = np.random.randn(N).astype(np.float32) c = np.empty_like(a)  a_gpu = cuda.mem_alloc(a.nbytes) b_gpu = cuda.mem_alloc(b.nbytes) c_gpu = cuda.mem_alloc(c.nbytes)  cuda.memcpy_htod(a_gpu, a) cuda.memcpy_htod(b_gpu, b)  lib.add_vectors(int(a_gpu), int(b_gpu), int(c_gpu), N)  cuda.Context.synchronize() cuda.memcpy_dtoh(c, c_gpu)  print(\"A:\", a) print(\"B:\", b) print(\"C = A + B:\", c) <pre>LIB:: ['_FuncPtr', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_func_flags_', '_func_restype_', '_handle', '_name']\nA: [ 0.78028464 -0.49346212 -0.90274906  0.9751807  -0.02011734  1.0545729\n  0.40566817  0.31163436 -0.9446583   0.56412727  0.51989985 -1.3264078\n -0.55833036  0.85947335 -0.4002817   1.0153143 ]\nB: [ 1.2956427   0.1763411   0.31895843 -1.928016    0.69085884 -1.1382663\n -1.5165892  -0.8581926  -0.6500315  -1.1406062   1.4036125   0.7908466\n -0.60482484 -0.04766817  0.4928366  -0.24710186]\nC = A + B: [-0.38582104  0.8457861   0.41153207 -1.2528391   0.59481716 -1.5670898\n -0.72116786 -0.39626685  2.7010136  -0.80919904  0.33222598  0.09432879\n  1.3553219  -0.07149178  1.6497035   0.14472684]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"00_The_Initiation/#phase-1-the-initiation","title":"Phase 1: The Initiation\u00b6","text":""},{"location":"00_The_Initiation/#step-1-gpu-architecture","title":"Step 1. GPU Architecture\u00b6","text":"<p>This will:</p> <ul> <li><p>Identify your GPU</p> </li> <li><p>Show you SM count (streaming multiprocessors)</p> </li> <li><p>Show warp size</p> </li> <li><p>Show max block/grid/thread limits</p> </li> <li><p>Reveal SIMT-style hints (like max threads per block)</p> </li> </ul>"},{"location":"00_The_Initiation/#step-2-cuda-programming-model-threads-blocks-grids-warps","title":"STEP 2: CUDA Programming Model \u2014 Threads, Blocks, Grids, Warps\u00b6","text":"<p>Next, let\u2019s illustrate how CUDA organizes parallelism using a kernel.</p> <p>We\u2019ll write a simple vector addition example that shows:</p> <ul> <li><p>How threads are indexed within a block and grid</p> </li> <li><p>The relationship between blocks, threads, and warps</p> </li> </ul>"},{"location":"00_The_Initiation/#step-3-compilation-runtime-nvcc-cu-device-vs-host-code","title":"STEP 3: Compilation &amp; Runtime \u2014 nvcc, .cu, Device vs Host Code\u00b6","text":"<p>Goals:</p> <ul> <li><p>Understand how CUDA code is compiled (separating host and device code)</p> </li> <li><p>See the relationship between .cu files, nvcc, and Python bindings</p> </li> <li><p>Compile a standalone .cu file and call it from Python</p> </li> </ul>"},{"location":"00_The_Initiation/#whats-really-going-on","title":"What\u2019s Really Going On?\u00b6","text":"<p>Host code: runs on the CPU (e.g., your Python or C++ control logic)</p> <p>Device code: runs on the GPU (your global kernels)</p> <p>nvcc separates and compiles them correctly, producing PTX or binary objects</p> <p>PyCUDA uses SourceModule() which auto-calls nvcc under the hood (in memory)</p>"},{"location":"00_The_Initiation/#lets-create-a-vector_addcu-file","title":"Let's create a vector_add.cu file\u00b6","text":""},{"location":"00_The_Initiation/#well-write-a-cu-file-and-compile-it-to-a-dynamic-linked-library-dll-then-call-it-from-python-using-ctypes","title":"We\u2019ll write a .cu file and compile it to a dynamic linked library (.dll) \u2014 then call it from Python using ctypes.\u00b6","text":"<p><code>nvcc -shared -o vector_add.dll vector_add.cu</code></p>"}]}