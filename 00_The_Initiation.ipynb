{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b56460-925e-4ff0-8325-4f6f6f33203c",
   "metadata": {},
   "source": [
    "We'll start by checking your GPU and printing basic architecture info using Python and pycuda.\n",
    "\n",
    "First, install PyCUDA if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4287c2-2905-4061-a285-61a7b36c2f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.7/1.7 MB 23.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pytools>=2011.2 (from pycuda)\n",
      "  Downloading pytools-2025.1.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in c:\\users\\iyeng\\miniconda3\\envs\\gpu_mode\\lib\\site-packages (from pycuda) (4.3.7)\n",
      "Collecting mako (from pycuda)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in c:\\users\\iyeng\\miniconda3\\envs\\gpu_mode\\lib\\site-packages (from pytools>=2011.2->pycuda) (4.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\iyeng\\miniconda3\\envs\\gpu_mode\\lib\\site-packages (from mako->pycuda) (3.0.2)\n",
      "Downloading pytools-2025.1.2-py3-none-any.whl (92 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml): started\n",
      "  Building wheel for pycuda (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycuda: filename=pycuda-2025.1-cp310-cp310-win_amd64.whl size=375812 sha256=0bc865bc5414920579b87239b7e27249eda2ef17f7b4f6426c5ae2e72c9ea971\n",
      "  Stored in directory: c:\\users\\iyeng\\appdata\\local\\pip\\cache\\wheels\\65\\53\\5f\\f5f184c26b7cc503acb77f3456531a6e1fac0ce30c774b9d82\n",
      "Successfully built pycuda\n",
      "Installing collected packages: pytools, mako, pycuda\n",
      "Successfully installed mako-1.3.10 pycuda-2025.1 pytools-2025.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86daa4ca-4a54-47a0-9893-6bbde57c6d03",
   "metadata": {},
   "source": [
    "# Step 1. GPU Architecture\n",
    "\n",
    "This will:\n",
    "\n",
    "- Identify your GPU\n",
    "\n",
    "- Show you SM count (streaming multiprocessors)\n",
    "\n",
    "- Show warp size\n",
    "\n",
    "- Show max block/grid/thread limits\n",
    "\n",
    "- Reveal SIMT-style hints (like max threads per block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3321e42d-8750-4fda-9a5c-4c03da119800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "Total Memory: 11.99 GB\n",
      "\n",
      "-- GPU Architecture Attributes --\n",
      "MULTIPROCESSOR_COUNT: 58\n",
      "MAX_THREADS_PER_BLOCK: 1024\n",
      "WARP_SIZE: 32\n",
      "MAX_BLOCK_DIM_X: 1024\n",
      "MAX_GRID_DIM_X: 2147483647\n",
      "CLOCK_RATE (KHz): 1830000\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "device = cuda.Device(0)\n",
    "attrs = device.get_attributes()\n",
    "\n",
    "print(f\"GPU Name: {device.name()}\")\n",
    "print(f\"Total Memory: {device.total_memory() / (1024 ** 3):.2f} GB\")\n",
    "print(\"\\n-- GPU Architecture Attributes --\")\n",
    "\n",
    "arch_attrs = {\n",
    "    \"MULTIPROCESSOR_COUNT\": cuda.device_attribute.MULTIPROCESSOR_COUNT,\n",
    "    \"MAX_THREADS_PER_BLOCK\": cuda.device_attribute.MAX_THREADS_PER_BLOCK,\n",
    "    \"WARP_SIZE\": cuda.device_attribute.WARP_SIZE,\n",
    "    \"MAX_BLOCK_DIM_X\": cuda.device_attribute.MAX_BLOCK_DIM_X,\n",
    "    \"MAX_GRID_DIM_X\": cuda.device_attribute.MAX_GRID_DIM_X,\n",
    "    \"CLOCK_RATE (KHz)\": cuda.device_attribute.CLOCK_RATE\n",
    "}\n",
    "\n",
    "for name, attr in arch_attrs.items():\n",
    "    print(f\"{name}: {attrs.get(attr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f62df0-7ff2-4e09-9b8a-1dc341e8aff1",
   "metadata": {},
   "source": [
    "# STEP 2: CUDA Programming Model — Threads, Blocks, Grids, Warps\n",
    "\n",
    "Next, let’s illustrate how CUDA organizes parallelism using a kernel.\n",
    "\n",
    "We’ll write a simple vector addition example that shows:\n",
    "\n",
    "- How threads are indexed within a block and grid\n",
    "\n",
    "- The relationship between blocks, threads, and warps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cddbff-87db-436b-8be7-76d82d33624e",
   "metadata": {},
   "source": [
    "This step demonstrates:\n",
    "\n",
    "- The CUDA thread hierarchy (grid, block, thread)\n",
    "\n",
    "- How to index threads globally\n",
    "\n",
    "- The connection between Python and CUDA C code\n",
    "\n",
    "- How to compile and launch kernels from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bddfa7-05f7-4763-8769-a0e931b26c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector A:\n",
      "[-0.09272769  0.36310634 -1.4122794  -1.531028   -1.5436966  -0.4410738\n",
      "  0.57584506  0.63177073  0.9921369  -1.0148718   1.544412   -0.6879888\n",
      "  0.1384869   0.90717006  0.20168625  0.22363763]\n",
      "\n",
      "Vector B:\n",
      "[-1.0652711   0.12170894  1.071717    1.1530142   1.4780273  -1.2505566\n",
      " -1.101104   -0.8431008   0.32481927 -0.58225757 -1.535062   -1.5423752\n",
      " -0.65916175 -0.44161093 -0.46648252  1.8605511 ]\n",
      "\n",
      "Vector C (A+B):\n",
      "[-1.1579988   0.4848153  -0.34056234 -0.37801385 -0.0656693  -1.6916304\n",
      " -0.52525896 -0.21133006  1.3169562  -1.5971293   0.00935006 -2.230364\n",
      " -0.5206748   0.46555912 -0.26479626  2.0841887 ]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iyeng\\AppData\\Local\\Temp\\ipykernel_31004\\3374425168.py:8: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu\n",
      "\n",
      "  mod = SourceModule(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "N = 16\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void add_vectors(float *a, float *b, float *c, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    if (idx < N) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "a = np.random.randn(N).astype(np.float32)\n",
    "b = np.random.randn(N).astype(np.float32)\n",
    "c = np.empty_like(a)\n",
    "\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "func = mod.get_function(\"add_vectors\")\n",
    "\n",
    "block_size = 4\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "\n",
    "func(a_gpu, b_gpu, c_gpu, np.int32(N), block=(block_size,1,1), grid=(grid_size,1))\n",
    "\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "print(f\"Vector A:\\n{a}\\n\")\n",
    "print(f\"Vector B:\\n{b}\\n\")\n",
    "print(f\"Vector C (A+B):\\n{c}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a98efd-2a05-4c27-896d-589d11a3a4a2",
   "metadata": {},
   "source": [
    "# STEP 3: Compilation & Runtime — nvcc, .cu, Device vs Host Code\n",
    "\n",
    "Goals:\n",
    "- Understand how CUDA code is compiled (separating host and device code)\n",
    "\n",
    "- See the relationship between .cu files, nvcc, and Python bindings\n",
    "\n",
    "- Compile a standalone .cu file and call it from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542a65b-705f-42d1-b802-2d3f787d3b55",
   "metadata": {},
   "source": [
    "## 4.1 What’s Really Going On?\n",
    "Host code: runs on the CPU (e.g., your Python or C++ control logic)\n",
    "\n",
    "Device code: runs on the GPU (your __global__ kernels)\n",
    "\n",
    "nvcc separates and compiles them correctly, producing PTX or binary objects\n",
    "\n",
    "PyCUDA uses SourceModule() which auto-calls nvcc under the hood (in memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccecaef-8260-473c-a9bc-107a925cf0e7",
   "metadata": {},
   "source": [
    "## Let's create a vector_add.cu file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001a181-4f88-42b1-9f65-39f6ba9b34dd",
   "metadata": {},
   "source": [
    "```\n",
    "// vector_add.cu\n",
    "extern \"C\" __global__ void add_vectors(float *a, float *b, float *c, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < N) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322e0f16-9d88-410c-ba6f-8ae4b9d6a65f",
   "metadata": {},
   "source": [
    "### We’ll write a .cu file and compile it to a dynamic linked library (.dll) — then call it from Python using ctypes.\n",
    "```nvcc -shared -o vector_add.dll vector_add.cu```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b0a83-55ba-41a3-a726-222247799c63",
   "metadata": {},
   "source": [
    "Note: That ```AttributeError: 'DeviceAllocation' object has no attribute 'handle'``` is because on Windows with PyCUDA, we don't use .handle to get the raw device pointer.\n",
    "\n",
    "Instead, use the int() cast, which gives you the actual pointer address in integer form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17d8ab-3c91-4154-9adc-dfd3d4e0d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIB:: ['_FuncPtr', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_func_flags_', '_func_restype_', '_handle', '_name']\n",
      "A: [ 0.78028464 -0.49346212 -0.90274906  0.9751807  -0.02011734  1.0545729\n",
      "  0.40566817  0.31163436 -0.9446583   0.56412727  0.51989985 -1.3264078\n",
      " -0.55833036  0.85947335 -0.4002817   1.0153143 ]\n",
      "B: [ 1.2956427   0.1763411   0.31895843 -1.928016    0.69085884 -1.1382663\n",
      " -1.5165892  -0.8581926  -0.6500315  -1.1406062   1.4036125   0.7908466\n",
      " -0.60482484 -0.04766817  0.4928366  -0.24710186]\n",
      "C = A + B: [-0.38582104  0.8457861   0.41153207 -1.2528391   0.59481716 -1.5670898\n",
      " -0.72116786 -0.39626685  2.7010136  -0.80919904  0.33222598  0.09432879\n",
      "  1.3553219  -0.07149178  1.6497035   0.14472684]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ctypes\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "N = 16\n",
    "block_size = 4\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "\n",
    "lib = ctypes.CDLL(\"./vector_add.dll\")\n",
    "print(\"LIB::\", dir(lib)) #Check the attributes of lib variable in Python\n",
    "# Define argument types for safety\n",
    "lib.add_vectors.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n",
    "\n",
    "a = np.random.randn(N).astype(np.float32)\n",
    "b = np.random.randn(N).astype(np.float32)\n",
    "c = np.empty_like(a)\n",
    "\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "lib.add_vectors(int(a_gpu), int(b_gpu), int(c_gpu), N)\n",
    "\n",
    "cuda.Context.synchronize()\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "print(\"A:\", a)\n",
    "print(\"B:\", b)\n",
    "print(\"C = A + B:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9203b4-ba3a-44c6-b7bb-78810e2287eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
